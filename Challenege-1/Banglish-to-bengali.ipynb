{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bn', 'rm']\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "\n",
    "# Load dataset\n",
    "dataset = load_dataset(\"SKNahin/bengali-transliteration-data\")\n",
    "\n",
    "# Split dataset\n",
    "train_test_split = dataset[\"train\"].train_test_split(test_size=0.2)\n",
    "train_data = train_test_split[\"train\"]\n",
    "val_data = train_test_split[\"test\"]\n",
    "print(train_data.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddbdb637666543969bf1d4969640a5ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4004 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing dataset: 'NoneType' object is not subscriptable\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e43d4c44ac34536a3fbb176c833b3bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1002 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing dataset: 'NoneType' object is not subscriptable\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1a0f09cf8e94798a862dbcdbdb4c491",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/4004 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "237892e47d924cfab522818d35a87da8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/1002 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing complete!\n"
     ]
    }
   ],
   "source": [
    "def clean_text(text, lang=\"banglish\"):\n",
    "    try:\n",
    "        if lang == \"banglish\":\n",
    "            text = re.sub(r\"[^a-zA-Z0-9\\s,.!?']\", \"\", text)  # Clean non-Banglish characters\n",
    "        elif lang == \"bangla\":\n",
    "            text = re.sub(r\"[^\\u0980-\\u09FF\\s,.!?']\", \"\", text)  # Clean non-Bangla characters\n",
    "        return text.strip()\n",
    "    except Exception as e:\n",
    "        print(f\"Error in clean_text: {e}\")\n",
    "        return \"\"  # Return empty string if an error occurs\n",
    "\n",
    "\n",
    "def tokenize_text(text, lang=\"banglish\"):\n",
    "    try:\n",
    "        if lang == \"banglish\":\n",
    "            return word_tokenize(text)  # Tokenize Banglish using word_tokenize\n",
    "        elif lang == \"bangla\":\n",
    "            return text.split()  # Simple split for Bangla\n",
    "        return text.split()  # Default behavior for unsupported languages\n",
    "    except Exception as e:\n",
    "        print(f\"Error in tokenize_text: {e}\")\n",
    "        return []  # Return an empty list if an error occurs\n",
    "\n",
    "\n",
    "def filter_sentence(sentence, min_len=3, max_len=50):\n",
    "    word_count = len(sentence.split())\n",
    "    return min_len <= word_count <= max_len\n",
    "\n",
    "\n",
    "def preprocess_data(example, idx):\n",
    "    try:\n",
    "        if idx == 999:  # Skip the 999th row (index 999)\n",
    "            return None\n",
    "\n",
    "        # Clean and tokenize Banglish (column 'rm')\n",
    "        example[\"rm\"] = clean_text(example[\"rm\"], lang=\"banglish\")\n",
    "        example[\"rm_tokens\"] = tokenize_text(example[\"rm\"], lang=\"banglish\")\n",
    "        \n",
    "        # Clean and tokenize Bangla (column 'bn')\n",
    "        example[\"bn\"] = clean_text(example[\"bn\"], lang=\"bangla\")\n",
    "        example[\"bn_tokens\"] = tokenize_text(example[\"bn\"], lang=\"bangla\")\n",
    "        \n",
    "        # Filter based on sentence length\n",
    "        if not filter_sentence(example[\"rm\"]) or not filter_sentence(example[\"bn\"]):\n",
    "            return None  # Skip this example if it's too short or too long\n",
    "        \n",
    "        return example\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing example at index {idx}: {e}\")\n",
    "        return None  # Return None if an error occurs\n",
    "\n",
    "\n",
    "# Assuming 'train_data' and 'val_data' are your datasets (from datasets library)\n",
    "\n",
    "def process_dataset(dataset):\n",
    "    try:\n",
    "        # Apply preprocessing, skip examples where None is returned, and pass indices\n",
    "        dataset = dataset.map(preprocess_data, remove_columns=dataset.column_names, with_indices=True, batched=False)\n",
    "        \n",
    "        # Filter out None values (if any) after mapping\n",
    "        dataset = dataset.filter(lambda x: x is not None)\n",
    "\n",
    "        return dataset\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing dataset: {e}\")\n",
    "        return dataset  # Return dataset in case of error to prevent failure\n",
    "\n",
    "# Process the training and validation datasets\n",
    "train_data = process_dataset(train_data)\n",
    "val_data = process_dataset(val_data)\n",
    "\n",
    "# Save preprocessed datasets\n",
    "train_data.save_to_disk(\"preprocessed_train_data\")\n",
    "val_data.save_to_disk(\"preprocessed_val_data\")\n",
    "\n",
    "print(\"Preprocessing complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bn': 'আচ্ছা ভাইয়া চেষ্টা করবো', 'rm': 'Assa vaiya chesta korbo'}\n",
      "{'bn': 'অ্যাপ এর নাম ঠিক করে লিখতে শিখেন না', 'rm': 'App er nam thik kore likhte shikhen na'}\n"
     ]
    }
   ],
   "source": [
    "print(train_data[0])\n",
    "print(val_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'MBart50Tokenizer'. \n",
      "The class this function is called from is 'MBartTokenizer'.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import MBartForConditionalGeneration, MBartTokenizer\n",
    "\n",
    "# Load mBART model and tokenizer\n",
    "model_name = \"facebook/mbart-large-50-many-to-many-mmt\"\n",
    "model = MBartForConditionalGeneration.from_pretrained(model_name)\n",
    "tokenizer = MBartTokenizer.from_pretrained(model_name)\n",
    "# Set the tokenizer's source and target languages (Banglish to Bangla)\n",
    "tokenizer.src_lang = \"en_XX\"  # Assuming Banglish is similar to English in its Romanized form\n",
    "tokenizer.tgt_lang = \"bn_BD\"  # Target language is Bangla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "536e01f5012643d9af836dfe3cd16ba8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4004 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "786d2538dc464966ba57ce0eed54ae9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1002 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize_function(examples):\n",
    "    # Tokenize both 'rm' (Banglish) and 'bn' (Bangla) columns\n",
    "    inputs = tokenizer(examples[\"rm\"], padding=\"max_length\", truncation=True, max_length=128)\n",
    "    targets = tokenizer(examples[\"bn\"], padding=\"max_length\", truncation=True, max_length=128)\n",
    "    inputs[\"labels\"] = targets[\"input_ids\"]\n",
    "    return inputs\n",
    "\n",
    "# Apply tokenization to the train and validation datasets\n",
    "train_data = train_data.map(tokenize_function, batched=True)\n",
    "val_data = val_data.map(tokenize_function, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",              # Where to save the model\n",
    "    evaluation_strategy=\"epoch\",         # Evaluate after each epoch\n",
    "    learning_rate=5e-5,                  # Learning rate\n",
    "    per_device_train_batch_size=8,       # Batch size for training\n",
    "    per_device_eval_batch_size=8,        # Batch size for evaluation\n",
    "    num_train_epochs=3,                  # Number of training epochs\n",
    "    weight_decay=0.01,                   # Weight decay for regularization\n",
    "    logging_dir=\"./logs\",                # Where to save logs\n",
    "    logging_steps=10,                    # Log every 10 steps\n",
    "    save_steps=500,                      # Save checkpoint every 500 steps\n",
    "    save_total_limit=2,                  # Keep only the last 2 checkpoints\n",
    ")\n",
    "\n",
    "# Define the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=val_data,\n",
    ")\n",
    "\n",
    "# Start fine-tuning\n",
    "trainer.train()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
